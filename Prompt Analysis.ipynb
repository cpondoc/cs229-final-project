{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3255412c",
   "metadata": {},
   "source": [
    "# Prompt Analysis\n",
    "Using NLP to determine which prompts our network got wrong to better understand which prompts are best for each diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3210eb",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934002a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For using NLTK later\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe9495",
   "metadata": {},
   "source": [
    "## Import Models\n",
    "For this case, we only import ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transferlearning import load_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d078795",
   "metadata": {},
   "source": [
    "## Import Dataset Class and Transforms\n",
    "Used to help load in the images for heatmap generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.dataset import ImageDataset\n",
    "from utilities.transforms import data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9869251",
   "metadata": {},
   "source": [
    "## Function to Determine Confusion Matrix\n",
    "The same framework as testing through the model, and then looking at prediction and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(transform, weights_path, batch_size, network, first, second):\n",
    "    # Loading in initial data\n",
    "    print(\"\\nLoading in data...\")\n",
    "    test_data = ImageDataset(\"test\", transform, 0.6, first, second)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    print(\"Done loading in data.\")\n",
    "\n",
    "    # Test Loading\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels, img_names = next(dataiter)\n",
    "\n",
    "    # Loading in a new example of the neural net, and loading in the weights\n",
    "    net = network\n",
    "    if (torch.cuda.is_available()):\n",
    "        net.to('cuda')\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "    \n",
    "    # Generate confusion matrix -- also count all real but fake images, and all images\n",
    "    print(\"Generate Confusion Matrix\")\n",
    "    matrix = [[0, 0], [0, 0]]\n",
    "    real_but_fake = []\n",
    "    all_images = []\n",
    "    \n",
    "    # Perform same as testing/inferencing, but logging data.\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels, paths = data\n",
    "            for i in range(len(paths)):\n",
    "                all_images.append(paths[i])\n",
    "            images_cuda, labels_cuda = images.cuda(), labels.cuda()\n",
    "            \n",
    "            # Feed the images through our network and evaluate them\n",
    "            outputs = net(images_cuda)\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            \n",
    "            # Calculate the right part of the matrix of prediction and actual\n",
    "            for i in range(len(predicted)):\n",
    "                prediction = int(predicted[i].item())\n",
    "                actual = int(labels[i].item())\n",
    "                matrix[prediction][actual] += 1\n",
    "                \n",
    "                # Adding to set of images to further analyze if real, but fake\n",
    "                if (prediction is 1 and actual is 0):\n",
    "                    real_but_fake.append(paths[i])\n",
    "    \n",
    "    return matrix, real_but_fake, all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650ec9",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "Runs all of the necessary functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73838e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_type, weights, first, second):\n",
    "    # Generate the necessary heatmaps\n",
    "    model = load_pretrained_model()\n",
    "    matrix, real_but_fake, all_images = generate_confusion_matrix(data_transforms[model_type], weights, 200, model, first, second)\n",
    "    \n",
    "    return matrix, real_but_fake, all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11c239",
   "metadata": {},
   "source": [
    "## Run all code!\n",
    "Runs all of the code for Transfer Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, real_but_fake, all_images = main(model_type = \"TransferLearning\", weights = 'weights/TransferLearning/stable-diffusion/TransferLearning-0.6.pth', first='stable-diffusion', second='real')\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77246693",
   "metadata": {},
   "source": [
    "## Function for Total Number of Nouns + Length\n",
    "Using NLTK to calculate total number of nouns and length (number of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_and_tokens(description):\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    num_nouns = counts['NN'] + counts['NNS'] + counts['NNP'] + counts['NNPS']\n",
    "    return num_nouns, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb0429",
   "metadata": {},
   "source": [
    "## Function for Linguistic Analysis\n",
    "Using the above function by looking at a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_analysis(all_images):\n",
    "    # Used to refer to mappings\n",
    "    df = pd.read_csv('dataset/reference.csv')\n",
    "    \n",
    "    # Keeping track of total nouns and lengths\n",
    "    total = 0\n",
    "    all_nouns = []\n",
    "    all_lengths = []\n",
    "    \n",
    "    # Iterate through each imags and calculate\n",
    "    for img in all_images:\n",
    "        # Grab the description\n",
    "        index = int(img[-9:-4])\n",
    "        description = df.iloc[index]['description']\n",
    "        \n",
    "        # Update nouns and tokens variables\n",
    "        num_nouns, tokens = nouns_and_tokens(description)\n",
    "        total += num_nouns\n",
    "        all_nouns.append(num_nouns)\n",
    "        all_lengths.append(len(tokens))\n",
    "    \n",
    "    # Print out statistics on number of nouns\n",
    "    print(\"Number of Nouns:\")\n",
    "    print(\"Mean: \" + str(np.mean(all_nouns)))\n",
    "    print(\"Variance: \" + str(np.var(all_nouns)))\n",
    "    print(\"\")\n",
    "    \n",
    "    # Print out statistics on lengths\n",
    "    print(\"Length of Message:\")\n",
    "    print(\"Mean: \" + str(np.mean(all_lengths)))\n",
    "    print(\"Variance: \" + str(np.var(all_lengths)))\n",
    "    print(\"\")\n",
    "    \n",
    "    return all_lengths, all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0154a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Function to Print Specific Nouns\n",
    "We can use this information to suggest an adversarial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_nouns(all_images):\n",
    "    # Used to refer to mappings\n",
    "    df = pd.read_csv('dataset/reference.csv')\n",
    "    \n",
    "    # Keeping track of all nouns and the tags for nouns\n",
    "    all_nouns = []\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    \n",
    "    # Iterate through each image\n",
    "    for img in all_images:\n",
    "        # Get the description\n",
    "        index = int(img[-9:-4])\n",
    "        description = df.iloc[index]['description']\n",
    "        \n",
    "        # Get the tokens and their tags\n",
    "        tokens = nltk.word_tokenize(description)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        \n",
    "        # Add to set if a noun\n",
    "        for word, tag in tagged:\n",
    "            if (tag in noun_tags):\n",
    "                all_nouns.append(word)\n",
    "    \n",
    "    return all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3163ea7",
   "metadata": {},
   "source": [
    "## Applying Linguistic Analysis Functions\n",
    "Calling above functions to print out data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aec3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing All Images:\")\n",
    "all_lengths, all_nouns = linguistic_analysis(all_images)\n",
    "\n",
    "print(\"\\nAnalyzing Real, but Fake Images:\")\n",
    "rbf_lengths, rbf_nouns = linguistic_analysis(real_but_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bd7db",
   "metadata": {},
   "source": [
    "## Super Awesome Bootstrapping Techniques!\n",
    "Ensuring statistical significance to make claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda371ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(total_observations, subsection_of_interest):\n",
    "    sample_mean = np.mean(total_observations)\n",
    "    mean_difference = abs(sample_mean - np.mean(subsection_of_interest))\n",
    "    subsection_length = len(subsection_of_interest)\n",
    "    count = 0.0\n",
    "    iteration_count = 10000\n",
    "    for _ in range(iteration_count):\n",
    "        sampled_lengths = np.random.choice(total_observations, subsection_length, replace=True)\n",
    "        if abs(np.mean(sampled_lengths) - sample_mean) >= mean_difference:\n",
    "            count += 1\n",
    "    print(count / iteration_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1e362",
   "metadata": {},
   "source": [
    "## Calculating Statistical Significance\n",
    "Calling above function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running simple bootstrapping to test against null hypothesis\")\n",
    "print(\"Statistical significance of prompt lengths\")\n",
    "bootstrapping(all_lengths, rbf_lengths)\n",
    "print(\"Statistical significance of noun counts\")\n",
    "bootstrapping(all_nouns, rbf_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c3a68",
   "metadata": {},
   "source": [
    "## Looking at Nouns in Real, but Fake\n",
    "See motivation above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looking at nouns of real, but fake images set:\")\n",
    "print(specific_nouns(real_but_fake))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
